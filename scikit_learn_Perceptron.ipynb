{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5QZqBrOwXDGyM1qZXitc/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/F-Palmieri/PRML_UPC/blob/main/scikit_learn_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Reference:\n",
        "\n",
        "     \n",
        "\n",
        "1.   Python machine learning : machine learning and deep learning with Python, scikit-learn, and TensorFlow. Raschka, Sebastian, autor; Mirjalili, Vahid, 2019 - Chapter 3\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# First steps with scikit-learn\n",
        "\n",
        "we will take a look at the **scikit-learn** API, which, as mentioned, combines a user-friendly and consistent interface with a highly optimized implementation of several classification algorithms. The scikit-learn library offers not only a large variety of learning algorithms, but also many convenient functions to preprocess data and to fine-tune and evaluate our models.\n",
        "\n"
      ],
      "metadata": {
        "id": "Rp6_MQueL_6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the Iris dataset from scikit-learn. Here, the third column represents the petal length, and the fourth column the petal width of the flower examples. The classes are already converted to integer labels where 0=Iris-Setosa, 1=Iris-Versicolor, 2=Iris-Virginica."
      ],
      "metadata": {
        "id": "IScQvYmGMfls"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axpr7rzZLzNC",
        "outputId": "fd64e5e0-47eb-43c8-9e4d-041c596de627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class labels: [0 1 2]\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, [2, 3]]\n",
        "y = iris.target\n",
        "\n",
        "print('Class labels:', np.unique(y))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *np.unique(y)* function returned the three unique class labels stored in\n",
        "*iris.target*, and as we can see, the Iris flower class names, Iris-setosa,\n",
        "Iris-versicolor, and Iris-virginica, are already stored as integers (here: 0, 1,\n",
        "2). \n",
        "To evaluate how well a trained model performs on unseen data, we will further\n",
        "split the dataset into separate training and test datasets. \n"
      ],
      "metadata": {
        "id": "CH9hn3e_M2kR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data into 70% training and 30% test data:\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=1, stratify=y)\n",
        "\n"
      ],
      "metadata": {
        "id": "_52V5Rb4NFmd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the `train_test_split` function already shuffles the training datasets\n",
        "internally before splitting. Via the `random_state` parameter, we provided a fixed random seed (`random_state=1`) for the internal pseudo-random number generator that is used for shuffling the datasets prior to splitting. Using such a fixed `random_state` ensures that our results are reproducible."
      ],
      "metadata": {
        "id": "74ITsRkmNPbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we took advantage of the built-in support for stratification via `stratify=y`. In this context, stratification means that the `train_test_split` method returns training and test subsets that have the same proportions of class labels as the input dataset. We can use NumPy's `bincount` function, which counts the number of occurrences of each value in an array, to verify that this is indeed the case:"
      ],
      "metadata": {
        "id": "ZjInZwGsNvFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Labels count in y:', np.bincount(y))\n",
        "print('Labels count in y_train:', np.bincount(y_train))\n",
        "print('Labels count in y_test:', np.bincount(y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bN57_I1NO73",
        "outputId": "5a6c59a9-be65-4fd3-d2d6-506e04e565db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels count in y: [50 50 50]\n",
            "Labels count in y_train: [35 35 35]\n",
            "Labels count in y_test: [15 15 15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fsZXQlH7M7wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Standardizing the features:**\n",
        "\n",
        "Here, we will standardize the features using the `StandardScaler` class from scikit-learn's preprocessing module:\n"
      ],
      "metadata": {
        "id": "GP1yso3cN-yF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "X_train_std = sc.transform(X_train)\n",
        "X_test_std = sc.transform(X_test)"
      ],
      "metadata": {
        "id": "zgZS6mppOGEX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the preceding code, we loaded the `StandardScaler` class from the\n",
        "`preprocessing` module and initialized a new `StandardScaler` object that we\n",
        "assigned to the *sc* variable. Using the `fit` method, `StandardScaler` estimated the parameters, **ùúá** (sample mean) and **ùúé** (standard deviation), for each feature dimension from the training data. By calling the `transform` method, we then standardized the training data using those estimated parameters, **ùúá**  and **ùúé**. Note that we used the same scaling parameters to standardize the test dataset so that both the values in the training and test dataset are comparable to each other."
      ],
      "metadata": {
        "id": "q6vy6bp7O1bc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sBvG_MaZO1Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a perceptron via scikit-learn"
      ],
      "metadata": {
        "id": "e4sW60jnOsIp"
      }
    }
  ]
}